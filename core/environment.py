# core/environment.py

import random
import logging

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

class Environment:
    """
    Entorno simb√≥lico para EvoAI que simula un espacio lineal con variables como energ√≠a,
    entrop√≠a y ruido contextual. Permite ejecutar acciones que afectan el estado del agente.
    """

    # Constantes de acci√≥n
    ACTION_EXPLORE = ("explore", "explorar")
    ACTION_ADVANCE = ("advance", "avanzar", "mover")
    ACTION_WAIT = ("wait", "esperar")
    ACTION_CALM = ("calm", "calmar")
    ACTION_RESET = ("reset", "reiniciar")

    def __init__(self, parameters=None):
        """
        Inicializa el entorno con par√°metros opcionales.

        Args:
            parameters (dict): Par√°metros como 'max_position'. Si no se proporciona, se usan valores por defecto.
        """
        self.parameters = parameters or {}
        self.max_position = self.parameters.get("max_position", 5)
        self.state = {}
        self.visited = set()
        self.reset()

    def reset(self):
        """
        Reinicia el entorno a su estado inicial.
        """
        self.state = {
            "pos": 0,
            "explorado": False,
            "entrop√≠a": 0.0,
            "energ√≠a": 100,
            "ruido": None
        }
        self.visited.clear()
        logger.info("üîÑ Entorno simb√≥lico reiniciado.")

    def observe(self):
        """
        Retorna una copia del estado actual del entorno.

        Returns:
            dict: Estado simb√≥lico del entorno.
        """
        return self.state.copy()

    def act(self, action):
        """
        Ejecuta una acci√≥n simb√≥lica y retorna la recompensa y estado de t√©rmino.

        Args:
            action (str): Acci√≥n simb√≥lica a ejecutar.

        Returns:
            tuple: (recompensa (float), finalizaci√≥n (bool))

        Raises:
            TypeError: Si la acci√≥n no es cadena de texto.
            ValueError: Si la acci√≥n es cadena vac√≠a.
        """
        if not isinstance(action, str):
            logger.error("La acci√≥n debe ser una cadena de texto.")
            raise TypeError("La acci√≥n debe ser una cadena de texto.")
        if not action:
            logger.error("La acci√≥n no puede ser una cadena vac√≠a.")
            raise ValueError("La acci√≥n no puede ser una cadena vac√≠a.")

        action = action.lower().strip()
        logger.debug(f"üé¨ Acci√≥n simb√≥lica recibida: {action}")

        reward = 0.0
        done = False
        current_pos = self.state["pos"]

        # Interpretaci√≥n de acci√≥n
        if action in self.ACTION_EXPLORE:
            if current_pos in self.visited:
                reward = -0.2
                logger.debug("üîÅ Exploraci√≥n redundante.")
            else:
                reward = 2.0
                self.state["explorado"] = True
                logger.info("üß≠ Exploraci√≥n nueva exitosa.")
                self.visited.add(current_pos)

        elif action in self.ACTION_ADVANCE:
            if current_pos < self.max_position:
                self.state["pos"] += 1
                self.state["energ√≠a"] -= 5
                reward = 1.0
                logger.info("üö∂‚Äç‚ôÇÔ∏è Movimiento hacia adelante.")
            else:
                reward = -0.5
                logger.warning("‚õî L√≠mite de movimiento alcanzado.")

        elif action in self.ACTION_WAIT:
            reward = -0.1
            logger.debug("‚è≥ Esperando...")

        elif action in self.ACTION_CALM:
            old_entropy = self.state["entrop√≠a"]
            self.state["entrop√≠a"] = max(0.0, old_entropy - 0.3)
            reward = 0.5 if old_entropy > 0.3 else -0.1
            logger.info(f"üßò Acci√≥n calmante: entrop√≠a de {old_entropy} ‚Üí {self.state['entrop√≠a']}")

        elif action in self.ACTION_RESET:
            self.reset()
            return 0.0, False

        else:
            reward = -1.0
            logger.warning(f"‚ùå Acci√≥n no reconocida: {action}")

        # Efectos secundarios: entrop√≠a y ruido (excepto para 'calm')
        if action not in self.ACTION_CALM:
            self.state["entrop√≠a"] = round(random.uniform(0.0, 1.0), 2)

        self.state["ruido"] = random.choice(["calma", "tensi√≥n", "caos", "arm√≥nico", "neutro"])

        # Condiciones de finalizaci√≥n
        if self.state["energ√≠a"] <= 0:
            logger.info("üíÄ Energ√≠a agotada.")
            done = True
        elif self.state["pos"] >= self.max_position:
            logger.info("üéØ Posici√≥n m√°xima alcanzada.")
            done = True
        elif self.state["explorado"]:
            logger.info("‚úÖ Exploraci√≥n completa.")
            done = True

        return reward, done
