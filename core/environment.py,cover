  # core/environment.py
  
! import random
! import logging
  
! logger = logging.getLogger(__name__)
! logging.basicConfig(level=logging.INFO)
  
! class Environment:
!     """
!     Entorno simb√≥lico para EvoAI que simula un espacio lineal con variables como energy,
!     entropy y noise contextual. Permite ejecutar acciones que afectan el state del agente.
!     """
  
      # Constantes de action
!     ACTION_EXPLORE = ("explore", "explorar")
!     ACTION_ADVANCE = ("advance", "avanzar", "mover")
!     ACTION_WAIT = ("wait", "esperar")
!     ACTION_CALM = ("calm", "calmar")
!     ACTION_RESET = ("reset", "reiniciar")
  
!     def __init__(self, parameters=None):
!         """
!         Inicializa el entorno con par√°metros opcionales.
  
!         Args:
!             parameters (dict): Par√°metros como 'max_position'. Si no se proporciona, se usan valores por defecto.
!         """
!         self.parameters = parameters or {}
!         self.max_position = self.parameters.get("max_position", 5)
!         self.state = {}
!         self.visited = set()
!         self.reset()
  
!     def reset(self):
!         """
!         Reinicia el entorno a su state inicial.
!         """
!         self.state = {
!             "pos": 0,
!             "explored": False,
!             "entropy": 0.0,
!             "energy": 100,
!             "noise": None
!         }
!         self.visited.clear()
!         logger.info("üîÑ Entorno simb√≥lico reiniciado.")
  
!     def observe(self):
!         """
!         Retorna una copia del state actual del entorno.
  
!         Returns:
!             dict: Estado simb√≥lico del entorno.
!         """
!         return self.state.copy()
  
!     def act(self, action):
!         """
!         Ejecuta una action simb√≥lica y retorna la recompensa y state de t√©rmino.
  
!         Args:
!             action (str): Acci√≥n simb√≥lica a ejecutar.
  
!         Returns:
!             tuple: (recompensa (float), finalizaci√≥n (bool))
  
!         Raises:
!             TypeError: Si la action no es cadena de texto.
!             ValueError: Si la action es cadena vac√≠a.
!         """
!         if not isinstance(action, str):
!             logger.error("La action debe ser una cadena de texto.")
!             raise TypeError("La action debe ser una cadena de texto.")
!         if not action:
!             logger.error("La action no puede ser una cadena vac√≠a.")
!             raise ValueError("La action no puede ser una cadena vac√≠a.")
  
!         action = action.lower().strip()
!         logger.debug(f"üé¨ Acci√≥n simb√≥lica recibida: {action}")
  
!         reward = 0.0
!         done = False
!         current_pos = self.state["pos"]
  
          # Interpretaci√≥n de action
!         if action in self.ACTION_EXPLORE:
!             if current_pos in self.visited:
!                 reward = -0.2
!                 logger.debug("üîÅ Exploraci√≥n redundante.")
!             else:
!                 reward = 2.0
!                 self.state["explored"] = True
!                 logger.info("üß≠ Exploraci√≥n nueva exitosa.")
!                 self.visited.add(current_pos)
  
!         elif action in self.ACTION_ADVANCE:
!             if current_pos < self.max_position:
!                 self.state["pos"] += 1
!                 self.state["energy"] -= 5
!                 reward = 1.0
!                 logger.info("üö∂‚Äç‚ôÇÔ∏è Movimiento hacia adelante.")
!             else:
!                 reward = -0.5
!                 logger.warning("‚õî L√≠mite de movimiento alcanzado.")
  
!         elif action in self.ACTION_WAIT:
!             reward = -0.1
!             logger.debug("‚è≥ Esperando...")
  
!         elif action in self.ACTION_CALM:
!             old_entropy = self.state["entropy"]
!             self.state["entropy"] = max(0.0, old_entropy - 0.3)
!             reward = 0.5 if old_entropy > 0.3 else -0.1
!             logger.info(f"üßò Acci√≥n calmante: entropy de {old_entropy} ‚Üí {self.state['entropy']}")
  
!         elif action in self.ACTION_RESET:
!             self.reset()
!             return 0.0, False
  
!         else:
!             reward = -1.0
!             logger.warning(f"‚ùå Acci√≥n no reconocida: {action}")
  
          # Efectos secundarios: entropy y noise (excepto para 'calm')
!         if action not in self.ACTION_CALM:
!             self.state["entropy"] = round(random.uniform(0.0, 1.0), 2)
  
!         self.state["noise"] = random.choice(["calma", "tensi√≥n", "caos", "arm√≥nico", "neutro"])
  
          # Condiciones de finalizaci√≥n
!         if self.state["energy"] <= 0:
!             logger.info("üíÄ Energ√≠a agotada.")
!             done = True
!         elif self.state["pos"] >= self.max_position:
!             logger.info("üéØ Posici√≥n m√°xima alcanzada.")
!             done = True
!         elif self.state["explored"]:
!             logger.info("‚úÖ Exploraci√≥n completa.")
!             done = True
  
!         return reward, done
